\subsection{Tailorable software}

\epigraph{End-User Development can be defined as a set of methods, techniques, and tools that allow users of software systems, who are acting as non-professional software developers, at some point to create, modify or extend a software artifact.
}{\textit{Lieberman et al.\cite{lieberman2006}}}

\subsubsection{Users as co-designers of their own tools}

In the context of End-User development an End-User is referred to as a person who is not an expert in Computer Science but uses a computer as a tool for performing activities relative to a given domain of expertise~\cite{lieberman2006}. With the development of technologies end-users have become more willing and demanding in terms of capability to tailor a software system to their own specific needs.\cite{Nardi1993}.

The increasing desire of end users to become information producers, to be able to shape their software tools and to participate in the design of their own product moves the design paradigm of a software systems toward an evolutionary and never-ending process where end-users become co-designers.\cite{Carmelo2011}

\subsubsection{Meta-design model}

A meta-design model aims at supporting adaptation to changes in a software system. It supports the creation of dynamic environments, adaptable to various contexts in which owners of diverse problems can act as designers. Instead of defining a finite product or content it rather provides the necessary tools that will enable end-users --according to their skills and culture-- to express a desired output.

A meta-design model defines different modes allowing its users to perform different activities adapted to their knowledge. (eg.  ``development mode'', ``design mode'', ``user mode''). In a user mode a 


\subsubsection{Transferability}

Transferability refers to ``the selection and application of a method in a development context'' as defined in~\cite{Carmelo2014}.

\subsection{Compiler and Interpreter Technologies}

A compiler is a program that takes some specifications described in a language (source code) as an input parameter and translates them into some given output where the output can be anything, from the translation of the specifications into another language (target code) or an immediate execution of them.

However, the term compiler typically applies in the specific case where the program transforms the source language into some target language and that the source language is usually of higher-level than the target language. When the target language is of higher-level than the source language, the term decompiler is commonly used and when the program translates from a high-level language into another high-level language it is referred to as a transpiler. A compiler is a special type of translator.

When a compiler immediately executes the source code containing the set of specifications the program is referred to as an interpreter. In this regard, this report describes an interpreter but it will also be referred to as a compiler with no distinction about the technical differences among those words.

An interpreter can execute the source code either by parsing and executing it directly or by translating it into some intermediate code that will then be executed. Another strategy is to execute pre-compiled procedures generated by a compiler and included them as a part of the interpreter.

Regardless of the program being a compiler or an interpreter, in both cases they rely on a ground structure comprised of a lexer and a parser.

\subsubsection{Lexical Analysis}

The lexical analysis is the first set of operations performed on the source code that is shipped into the compiler as a simple string of text where it is divided into a meaningful set of tokens. A token is a lexeme (unit of lexical meaning) with a given type and value.

The program performing lexical analysis is referred to as a tokenizer or as a lexer.
Lexing is generally subdivided into two phases: 
\begin{itemize}
    \item The \emph{scanning} phase, which is based on a finite-state machine where each state holds a defined set of rules describing the sequence of characters that can possibly occur in a lexeme. The next current-state to move into is typically inferred by looking at the first none-white character. Together with that initial character, all following characters until reaching a character which does not match the defined rules for the given state constitutes the lexeme's value. 
    \item The \emph{evaluation} phase, which is where the lexeme, which is until then a simple string is assigned a given type. Together with the value this constitutes a token.
\end{itemize}

Lexing is generally done as a single pass over the source code, looking up each character of the string exactly one time. Regular expressions are useful to express patterns that the characters in the lexeme must follow.

The lexer relies on a component that keeps track on the current position in the text string and that exposes methods for reading the next character without moving the pointer and for reading the next character but moving the pointer one step forward. In the same way, the set of tokens resulting from the lexical analysis will typically be exposed to the parser through an interface allowing to read the tokens through the same set of operations.

\subsubsection{Parsing}

The parser is a component of the compiler that, based on a set of tokens, builds a data structure --usually an abstract syntax tree (AST)-- that forms the internal representation of the program. It operates syntactic analysis following the production rules of a formal grammar. A formal grammar in itself does not say anything about the meaning of the set of symbols that composes a string. Through the parsing process, most often thanks to its resulting AST, the parser enables a program to make ``use'' of the source code, to give it a specific semantic ``meaning''. The implementation of a parser heavily depends on the syntax of the language.

\todo[inline]{more about formal languages ??}

\subsubsection{Interpretation}

In the specific case of interpreters, where the source code is immediately interpreted, the program directly uses the data structure that has been generated by the parser. What the interpreter actual does is unique to the very specific problem to solve.

\subsection{JavaScript}

In this chapter I speak about JavaScript as a language, the eco-system around it and I introduce various concepts that have all been subject to their own sets of literature. My intention is not to draw a full fledged explanation about these concepts but rather introduce them as an underlying background and justification for the decisions I have taken to support the implementation that I describe later on.

\subsubsection{Brief history}

Javascript was created by Brendan Eich in May 1995 at Netscape. It's name has evolved from Mocha to LiveScript and then finally JavaScript. Netscape navigator 2.0 was the first browser to be released with support for that language. The ECMAScript specification refers to the standardization of JavaScript that was taken to ECMA International in 1997 with the intention of defining a standard specification which other browser vendors could then implement. With Internet Explore 3.0 in August 1996, Microsoft featured support for JavaScript through JScript, their own version of a compatible dialect that was named differently for licensing reasons. The early years of JavaScript are marked by disagreements on the standards between corporations, especially between Microsoft and Netscape which both supported ECMAScript but where both added different features not described in the specification.

In 2005, Jesse James Garrett coined the term ``Ajax''\cite{Garrett2005}, a shorthand for Asynchronous JavaScript + XML that involves a set of technologies allowing to asynchronously load data in the background, without the need of refreshing the entire page. This has been a main event in the history of JavaSript because it suddenly enabled to create richer and dynamic web applications. This has resulted in the language quickly gaining in popularity with a massive support from the open-source community that contributed to the language with mainstream libraries such as jQuery and Prototype.

Over time, because it is the only client-side language supported by browsers JavaScript has become the standard and only language for doing web-development. Besides it's very lightweight specification the language comes with it's own set of particularities that can make it unintuitive to use.

\subsubsection{Asynchronous Calls}

Asynchronous execution of some task usually occurs in a multi-threading system when a thread that initiates the task does not wait for that task to complete as opposed to a synchronous execution. Browsers do not provide access to a threading model. Everything running within a browser runs on the same single thread.

In the context of browsers the term asynchronous is rather used because the execution flow of a program will not block while waiting for the response of a request to a particular resource. Ajax calls are good examples of actions triggering such asynchronous calls. Applications can take advantage of a browser's API which allows asynchronous programming thanks to events or callbacks. The XMLHttpRequest that helps supporting Ajax calls is a good example of a service using both as shown in~\ref{lst:xhr}. In the same way as the for the browser's asynchronous API, an application should be designed to enable asynchronous handling.

\begin{lstlisting}[
    caption={Example of the XMLHttpRequest supporting asynchronous programming both through an event-based and a callback method},
    label={lst:xhr}
]

var xhr = new XMLHttpRequest();
xhr.addEventListener("load", 
  function () {// do something}
);

xhr.onreadystatechange = function () {
  // do something;
}

xhr.open("GET", "http://www.example.com/...");
xhr.send();
\end{lstlisting}

\paragraph{Callbacks}

A callback relies on the fairly simple principle that a function to apply the result of an asynchronous operation is passed as a parameter to the caller function. This ``inner'' representation of asynchronous code quickly makes readability a serious concern for two reasons:
\begin{itemize}
    \item The logical execution order of the code is not linear which means that it is not executed in the order in which it is written. This is particularly true when performing asynchronous calls depending on other ones.
    \item The second reason which is a direct consequence of the previous is what happens when trying to write the instructions in the logical order at which they will be executed. It causes functions to be nested always deeper within the initial one, causing the indentation to shift more and more to the right.
\end{itemize}

This problem is so common that it is often referred to as the ``callback hell'' and some good-practice guidelines have been written in order to help developers to write better asynchronous JavaScript programs\cite{CallbackHell}.

Asynchronous programming in JavaScript also requires to address the question of error handling properly. Since errors are most likely to appear in a different ``context'', surrounding a function that initiates an asynchronous call with a try/catch has no effect in case of failure. In order to handle errors, a function that can be called if an operation fails can be passed as a second parameter to the caller.

\begin{lstlisting}[
    caption={Example of asynchronous call},
    label={lst:async}
]

getResource("file1", function (stream1) {
    var file2 = "..."; // get file address from content in stream1
    getResource(file2, function (stream2) {
        // stream1 and stream2 are available here
        var schema = "..."; // get the schema address from content in stream2
        getResource(stream1, function (schema) {
            // stream1, stream2 and schema are available here
            // All resources are ready to perform some arbitrarty task
        }, function (err) {
            console.log("error schema:", err);
        });
    }, function (err) {
        console.log("error:", err);
    });
}, function (err) {
    console.log("error:", err);
});
\end{lstlisting}

Listing~\ref{lst:async} shows an example of a program that relies on 3 different files in order to perform some arbitrary task. The files can be retrieved through the asynchronous function ``getResource''. It takes a string and a callback function as parameters.

\begin{lstlisting}[
    caption={Waiting for multiple asynchronous calls to complete},
    label={lst:asyncMany}
]

var counter = 10;
var data = [];

function getRandomInt(min, max) {
  return Math.floor(Math.random() * (max - min)) + min;
}

for (var i = 0, len=10; i < len; i++) {
    setTimeout(function() {
        // success handler

        // save response
        data.push(getRandomInt(0, 10));

        // check if last async call
        if (--counter <= 0) {
            // all data is ready and is store in data[]
            // do some operation.
        }
    }, getRandomInt(100, 1000));
}
\end{lstlisting}

When waiting for multiple asynchronous calls to fulfill, the only and one strategy with callbacks is to maintain a global counter and to change and check withing the callback function --which is executed when an operation completes-- the current state of this counter. Listing \ref{lst:asyncMany} shows a concrete implementation of how to wait multiple asynchronous calls to complete before doing some operations which can use the resulting data.

\paragraph{Promise}

``Promises'' --also referred to as a futures or a defers-- is a pattern coined in 1976~\cite{Friedman1976} that can be used in JavaScript to greatly improve code readability. It is implemented as a construct used to represent the value that will be available as a result to an asynchronous call.

In a synchronous context, a function returns a value or throw exceptions. When performing some asynchronous task within the body of a function a callback is used to handle the result. In this case, the function returns nothing. Processing the result and any eventual failure takes place ``internally'' to the function.

A promise is an object representing the value which is immediately returned to the caller regardless of the fact that the value is ready or will be ready at a later point in time. It is a mechanism that proposes to handle the result of an asynchronous operation ``externally'' rather than within the body of the caller function. The promise can then become either:
\begin{itemize}
    \item fullfiled with a resutling value;
    \item or rejected with a given exception;
\end{itemize}

Concretely it gives to a function performing asynchronous operations a behaviour similar to that of a function performing synchronous operations where either a value is expected to be immediately returned otherwise an exception is thrown. By taking advantage of promises, code gains in readability as the chain of asynchronous functions can be flattened out. Error handling becomes easier because if the chain is interrupted at any time by a rejection, the error is bubbled up the chain. Listing~\ref{lst:promise} shows an example of a program doing the same operations as in listing~\ref{lst:promise} but using promises.

\begin{lstlisting}[
    caption={Example of a promise chain},
    label={lst:promise}
]

getResource("file1")
    .then(function (stream1) {
        var file2 = "..."; // get file address from content in stream1
        return getResource(file2);
    })
    .then(function (stream2) {
        // stream2 is available here
        var schema = "..."; // get the schema address from content in stream2
        return getResource(schema);
    })
    .then(function (schema) {
        // schema available here
    })
    .catch(function (err) {
        console.log("error": err);
        // perform some arbitrarty task
    });

\end{lstlisting}


\begin{lstlisting}[
    caption={Example of a promise chain},
    label={lst:promiseAll}
]
var data = [];

function getRandomInt(min, max) {
  return Math.floor(Math.random() * (max - min)) + min;
}

for(var i=0, len=10; i<len; i++) {
    data.push(new Promise(function (resolve, reject) {
        setTimeout(resolve, getRandomInt(100, 1000), getRandomInt(0, 10));
    }));
}

Promise.all(data).then(function(randomValues) {
    // all data is ready
});

\end{lstlisting}

Promises also provides a efficient way to handle the case where an operation needs to wait for multiple asynchronous calls through the ``all'' method as shown in listing~\ref{lst:promiseAll}



\subsubsection{Prototypical inheritance}

Javascript uses prototypal inheritance which is a specific form of object-oriented code-reuse.

In classical inheritance, objects are described through classes. A class holds some ``template-code'' describing an object. Multiple objects can be instantiated from the same class. Classes can be organized into a hierarchical structure where more general code is store at the top of this hierarchy and gets more specialised as we go down the hierarchy. Different objects instantiated from the same class share the same code and objects share the same code as the objects they extend.

In prototypal inheritance, there is no classes. All objects have a prototype which they inherit all their properties and methods from. Instead of requesting an instance of a given class, objects can be created by literally writing them. In order to create a prototype object a ``constructor'' function can be invoked with the ``new'' keyword as shown in listing~\ref{lst:constructor}. It is worth noting a few things here:
\begin{itemize}
    \item ``JavaScript is designed on a simple object-based paradigm''\cite{mozilla}. It means that everything is an Object, functions are also objects. Since objects can have properties, and properties can have values, the same applies for functions. In the listing, ``prototype'' is a property of the ``constuctor'' function ``MyObject''
    \item \texttt{new SomeObject} produces a new Object that inherits --getting all the properties and methods-- from SomeObject.prototype.
\end{itemize}

\begin{lstlisting}[
    caption={Creating an object using the ``new'' keyword},
    label={lst:constructor}
]
function SomeObject(field) {
    this.field = field;
}

SomeObject.prototype.getValue() {
    return this.value;
}

var myInstance = new SomeObject("value");

console.log(myInstance.field);        // --> "value"
console.log(myInstance.getValue());   // --> "value"
\end{lstlisting}

Douglas Crockford has extensively written about simulating a classical inheritance system with the ``expressive power'' of JavaScript~\cite{crockford_inheritance} and his work has been a reference in the domain. Later, he has published another white paper in which he claims to have ``liberated'' himself from the classical system and to have ``learned to fully embrace prototypalism''\cite{crockford_prototypal}. His reflections through this later publication have actually been integrated as part of the ECMAScript 5 specification, through the \texttt{Object.create()} which provides an alternative way to create a new Object with a given prototype and set of properties. The internal logic of Object.create() can be seen in listing~\ref{lst:objectCreate}. About supporting classical inheritance in Javascript he says himself: ``I now see my early attempts to support the classical model in JavaScript as a mistake.''.

\begin{lstlisting}[
    caption={Internal logic behind ECMAScript5's Object.create},
    label={lst:objectCreate}
]
function object(o) {
    function F() {}
    F.prototype = o;
    return new F();
}
\end{lstlisting}

Later in the implementation I show how to practically use prototypal inheritance for supporting code-reuse between the different graphical components of the system that I will also describe.

\subsubsection{Module Pattern}

The module pattern\cite{Miraglia2007} has become a central concept. It helps to encapsulate the program's state within the scope of an anonymous function. Listing~\ref{lst:modulePattern} shows naïve implementation of the module pattern.

\begin{lstlisting}[
    caption={Module pattern in Javascript},
    label={lst:modulePattern}
]
var module = (function () {

  var service = {};
  var myPrivate = 2;
  
  service.myPublic = function () {
    console.log("Public method")
  } 

  return service;

}());

module.myPublic();
\end{lstlisting}

Since 2007, creative contributions have been made around this pattern ultimately leading to different module formats. The Asynchronous Module Definition (AMD) was the first to support a standard way to modularize an application in a browser. It specifies a method to define how to encapsulate code into useful units and a method to refer to dependencies located in other units. AMD requires a to refer to a module loader --a whole Javascript application on its own-- as a dependency to an application. RequireJS is one of such module loader. At the same time, Node.js --a server side JavaScript engine-- implemented CommonJS, it's own module format, which shares a common goal with AMD but is meant to be used on a server and not in a browser. Tools such as browserify quickly made it possible however to create a ``JavaScript bundle'', a transformed version of some JavaScript source code, which would enable to write programs following the CommonJS module format and port them into the browser. 

CommonJS, together with a complex eco-system offers powerful possibilities in terms of organization of a solid development environment similar to what can be found in other languages such as Java etc.

The concept of module formats has been integrated as a part of the ECMAScript 2015 specification and JavaScript now includes it's own module pattern, taking over the gap that both AMD and CommonJS were trying to resolve.

ECMAScript 2015 is not yet supported by all browsers. However, the eco-system around JavaScript (WebPack, Babel etc.) makes it simple to transpile source code from newer to older dialects of JavaScript. In my implementation I have used this later strategy.

\subsubsection{Programming Style}

Javascript is a flexible language that supports a variety of programming styles. Listing~\ref{lst:objectOrientedJS} and listing~\ref{lst:functionalJS} are examples of programs structured in two different ways but solving the very same task of applying a transformation with a given value on each numbers in an array. 


\lstinputlisting[
    caption={object-oriented structure},
    label={lst:objectOrientedJS},
    firstline=3,
    lastline=26
]{code/JSstyles.js}

\lstinputlisting[
    caption={functional based structure},
    label={lst:functionalJS},
    firstline=31,
    lastline=45
]{code/JSstyles.js}


Listing~\ref{lst:objectOrientedJS} follows an object-oriented approach with the ``Transformer'' function being used as a constructor function to mimic a class. A new ``Tranformer'' object is instantiated using the ``new'' keyword that: creates a new object inheriting all properties from the constructor's prototype, calls the constructor and binds ``this'' to the newly created object and finally returns the object. It means that each instantiated object has the set of methods declare on the constructor function's prototype and they allow to execute some operations on the object. Similarly, the object contains all the properties declared in the constructor function and associated to the object to instantiate (``this''). When invoked, the methods of the object are depending on the actual properties defined on the current object.

JavaScript treats function as first-class objects meaning that they can be dynamically created, destroyed and passed to other functions in the program at runtime. Listing~\ref{lst:functionalJS} uses this intrinsic property of the language to apply transformations in the array in purely functional way where action to perform is passed as an argument into another function doing the actual transformation on the array.

The flexibility of JavaScript makes it a powerful language but combined with it's evolution over the years, it's many design patterns and it's surrounding eco-system makes it a language that is not trivial to grasp.

\subsubsection{Compilers in JavaScript}

As a first step in my approach to write an interpreter for the domain specific language specific to visTool, I have looked for related works and have found two good technical white papers describing how to write an interpreter in JavaScript~\cite{crockford2007}\cite{bazon16}.

I have chosen to follow the approach of Mihai Bazon who proposes a method that nicely uses the built-in abilities of JavaScript through a functional way of writing the interpreter. His work relies on the premise that functions can be passed to other function in the same way as in the example in listing~\ref{lst:functionalJS}.

His principle is simple and I will show later on, through the implementation, that during the lexical analysis, a function is used to hold the production rules of a given state of the finite-state machine that composes the lexer. Each function has the possibility of reading the next character in the input string (the program) and accordingly a token can be evaluated an returned to the caller function. The parser follows the same strategy.