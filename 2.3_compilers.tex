\subsection{Compiler and Interpreter Technologies}

A compiler is a program that takes some specifications described in a language (source code) as an input parameter and translates them into some given output where the output can be anything, from the translation of the specifications into another language (target code) or an immediate execution of them.

However, the term compiler typically applies in the specific case where the program transforms the source language into some target language and that the source language is usually of higher-level than the target language. When the target language is of higher-level than the source language, the term decompiler is commonly used and when the program translates from a high-level language into another high-level language it is referred to as a transpiler. A compiler is a special type of translator.

When a compiler immediately executes the source code containing the set of specifications the program is referred to as an interpreter. In this regard, this report describes an interpreter but it will also be referred to as a compiler with no distinction about the technical differences among those words.

An interpreter can execute the source code either by parsing and executing it directly or by translating it into some intermediate code that will then be executed. Another strategy is to execute pre-compiled procedures generated by a compiler and included them as a part of the interpreter.

Regardless of the program being a compiler or an interpreter, in both cases they rely on a ground structure comprised of a lexer and a parser.

\subsubsection{Lexical Analysis}

The lexical analysis is the first set of operations performed on the source code that is shipped into the compiler as a simple string of text where it is divided into a meaningful set of tokens. A token is a lexeme (unit of lexical meaning) with a given type and value.

The program performing lexical analysis is referred to as a tokenizer or as a lexer.
Lexing is generally subdivided into two phases: 
\begin{itemize}
    \item The \emph{scanning} phase, which is based on a finite-state machine where each state holds a defined set of rules describing the sequence of characters that can possibly occur in a lexeme. The next current-state to move into is typically inferred by looking at the first none-white character. Together with that initial character, all following characters until reaching a character which does not match the defined rules for the given state constitutes the lexeme's value. 
    \item The \emph{evaluation} phase, which is where the lexeme, which is until then a simple string is assigned a given type. Together with the value this constitutes a token.
\end{itemize}

Lexing is generally done as a single pass over the source code, looking up each character of the string exactly one time. Regular expressions are useful to express patterns that the characters in the lexeme must follow.

The lexer relies on a component that keeps track on the current position in the text string and that exposes methods for reading the next character without moving the pointer and for reading the next character but moving the pointer one step forward. In the same way, the set of tokens resulting from the lexical analysis will typically be exposed to the parser through an interface allowing to read the tokens through the same set of operations.

\subsubsection{Parsing}

The parser is a component of the compiler that, based on a set of tokens, builds a data structure --usually an abstract syntax tree (AST)-- that forms the internal representation of the program. It operates syntactic analysis following the production rules of a formal grammar. A formal grammar in itself does not say anything about the meaning of the set of symbols that composes a string. Through the parsing process, most often thanks to its resulting AST, the parser enables a program to make ``use'' of the source code, to give it a specific semantic ``meaning''. The implementation of a parser heavily depends on the syntax of the language.

\todo[inline]{more about formal languages ??}

\subsubsection{Interpretation}

In the specific case of interpreters, where the source code is immediately interpreted, the program directly uses the data structure that has been generated by the parser. What the interpreter does is unique to the very specific problem that needs to be solve. This project aims at showing how to interpret different text files in order to build a graphical user-interface. I introduce in greater details the specificity of the problem in the context of visTool in section~\ref{sec:languageDescription}.

The notion of scope or environment is important to stress as I will use it again later. It is the part of the program where a binding between a name and an entity is valid. While interpreting the scope holds all the objects that can be addressed.